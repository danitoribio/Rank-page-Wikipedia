{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f08c1a2-46e8-4db0-bcfc-e866e9b5df3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Wikipedia Analysis\n",
    "\n",
    "#### Daniel Toribio (100454242) \n",
    "\n",
    "This last practical work is focused on putting all our skills together, in order to develop a page rank algorithm on the wikipedia dataset. For this purpose, we will follow a number of steps:\n",
    "\n",
    "* Import the data \n",
    "* Filter the data to keep only essential variables (title, id, text).\n",
    "* Create a pipeline to extract the forward links of each document (in the form of its ID). Used to create *Forward Table* \n",
    "* Use *Forward Table* to calculate number of outgoing links per document\n",
    "* Use *Forward Table* to create a *Reverse Table*\n",
    "* Initialize *Page Rank Table*\n",
    "* Loop through the page rank update algorithm until convergence. This will be when:\n",
    "  * Max number of iterations is reached (20)\n",
    "  * Page rank update magnitude is lower than the threshold we have defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb68944e-543a-439f-bffb-4f0bc5f1b267",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Reading the data\n",
    "\n",
    "The database comprises 5.823.210 entries, organized in a set of parquet files. For the initial functional version, we'll assess our algorithm on a subset of 0.01% of records, roughly 580 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9990af17-2206-46cd-983a-b3538c4dfa1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import ArrayType, StringType,LongType\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223ccab3-ee90-4d99-9f3a-820fab4a6cb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WikipediaDF = spark.read.parquet(\"dbfs:/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet\") #Here we create a Spark DataFrame with all the Wikipedia entries\n",
    "\n",
    "PartialWikipediaDF = WikipediaDF.sample(fraction=0.01,seed=0).cache() #Here we take a fraction of the Wikipedia Spark Dataframe to avoid using a big number of entries and have lower execution time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8d8ebc-0091-4564-9cf7-46172330e6a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PartialWikipediaDF.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d623a89-9a17-49eb-8bdf-20821145228f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As can be seen above, the original dataset has many variables that will not be used in our analysis. We are only interested in 3 of them, which are the **title** of the document, its **id**, and its **text**. It is from the latter that we will extract the forward links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3900feeb-5fea-4eb1-86f8-d82e8a66be00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Parsing the Data\n",
    "\n",
    "Up until this step, we have all the links information stored in the variable **text**, where the outgoing links are identified if they are enclosed within \"[[]]\". Inside this, we find the title of the document it is referring to. Therefore, in order to create a dataframe with the desired structure, there are 2 main steps to execute:\n",
    "* Parse the text to find all linked document titles\n",
    "* Map the document titles to their id\n",
    "\n",
    "### 2.1 Extracting linked document titles\n",
    "\n",
    "This will simply be done by creating a new user defined function, that given the text, uses regular expressions to find all text enclosed in double '[[]]' marks. We add some extra conditions to filter out undesired results, such as image or file attachments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a2075a5-ab03-47c3-8fc2-b10d35c8760d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_links(document_body):\n",
    "    '''\n",
    "    Recieves text field and parses it to only keep references to other articles (enclosed between [[]])\n",
    "    Some bugs with attched files and references, as also use [[]]. \n",
    "    Ej:  [[File:Carl XCH-4.jpg|thumb|The [[United States Navy|U.S. Navy's]] ''XCH-4'', with hydrofoils clearly lifting the hull out of the water]] \n",
    "    For these cases, we search for reference inside these substrings ([[United States Navy|U.S. Navy's]] in this case)\n",
    "    If none are found, we delete this entry to our list of references\n",
    "    \n",
    "    INPUT: \n",
    "        - text: string to be parsed\n",
    "        \n",
    "    OUTPUT:\n",
    "        - references: a list of titles of references entries\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #Finds initial [[]]\n",
    "    ref = re.findall(\"\\[\\[(.+?)\\]\\]\", document_body)\n",
    "\n",
    "    # Sometimes for file and image attachments some [[]] are used, but no citation. If the citation has no internal references, remove too.\n",
    "    del_ = []\n",
    "    for i in range(len(ref)):\n",
    "        ref[i] = ref[i].lower()\n",
    "        if '[[' in ref[i]:\n",
    "            #Search the internal references in file attachments\n",
    "            ref[i] = re.search(\"\\[\\[(.*)\", ref[i]).group()[2:]\n",
    "        if ref[i].startswith(('file:', 'image:', 'category:', ':')):\n",
    "            #If plain file/image entry, delete\n",
    "            del_.append(i)\n",
    "\n",
    "    #If we do not sort in descending order, indexing will give errors as dimensions change\n",
    "    del_.sort(reverse=True)\n",
    "    for i in del_:   \n",
    "        del ref[i]\n",
    "\n",
    "    return ref\n",
    "\n",
    "parse_links_udf = udf(parse_links,ArrayType(StringType())) #Here we create the UDF which parses the text field from each record, and extracts the outgoing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67dbaa28-71f2-4db3-8ae0-859d823336cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## This is a test to see the correct functionality of the code\n",
    "\n",
    "test=\"{{Use Indian English|date=April 2015}} {{Infobox person | name = Shavez Khan | image = | caption = | birth_date = | birth_place = India | nationality = India | residence = [[Mumbai]], India | occupation = [[Actor]] | years_active = present | height = }} '''Shavez Khan''' is an [[India]]n television [[actor]]. He has done his roles in various Indian television shows like Shaitaan,<ref>{{cite web|url=http://www.tellychakkar.com/tv/tv-news/shavez-khan-feature-episodic-of-colors-shaitaan|title=Shavez Khan to feature in an episodic of Colors' Shaitaan|work=Tellychakkar|date=11 April 2013|accessdate=24 April 2015}}</ref> [[Encounter (Indian TV series)|Encounter]], [[Ek Hasina Thi (TV series)|Ek Hasina Thi]], [[Savdhaan India]],<ref>{{cite web|url=http://www.tellychakkar.com/tv/tv-news/shavez-khan-anshul-singh-and-damini-joshi-episodic-of-savdhan-india-140915|title=Shavez Khan, Anshul Singh and Damini Joshi in an episodic of Savdhan India|work=Tellychakkar|date=15 September 2014|accessdate=24 April 2015}}</ref> [[SuperCops vs Supervillains]],<ref>{{cite web|url=http://www.tellychakkar.com/tv/tv-news/rituraj-singh-and-shavez-khan-life-oks-shapath-141009|title=Rituraj Singh and Shavez Khan in Life OK's Shapath|work=Tellychakkar|date=9 October 2014|accessdate=24 April 2015}}</ref> Pyaar Ka The End,<ref>{{cite web|url=http://www.tellychakkar.com/tv/tv-news/shavez-khan-bindass-pyaar-ka-the-end-141029|title=Shavez Khan in Bindass' Pyaar Ka The End|work=Tellychakkar|date=29 October 2014|accessdate=24 April 2015}}</ref> [[Pyaar Kii Ye Ek Kahaani]], [[MTV Fanaah]], [[Crime Patrol (TV series)|Crime Patrol]]. He has played his recent role in [[Sony Entertainment Television (India)|Sony TV]]'s [[C.I.D. (Indian TV series)|CID]].<ref>{{cite web|url=http://www.tellychakkar.com/tv/tv-news/shavez-khan-sony-tvs-cid-150417|title=Shavez Khan in Sony TV's CID|work=Tellychakkar|date=17 April 2015|accessdate=24 April 2015}}</ref> ==Television== *[[Colors (TV channel)|Colors]]'s Shaitaan *[[Sony Entertainment Television (India)|Sony TV]]'s [[Encounter (Indian TV series)|Encounter]], [[Crime Patrol (TV series)|Crime Patrol]] & [[C.I.D. (Indian TV series)|CID]] *[[Star Plus]]'s [[Ek Hasina Thi (TV series)|Ek Hasina Thi]] *[[Life OK]]'s [[Savdhaan India]] & [[SuperCops vs Supervillains]] *[[Bindass]]' Pyaar Ka The End *[[Star One]]'s [[Pyaar Kii Ye Ek Kahaani]] *[[MTV]]'s [[MTV Fanaah]] ==References== {{Reflist}} ==External links== {{Persondata | NAME = Khan, Shavez | ALTERNATIVE NAMES = | SHORT DESCRIPTION = Indian model and television actor | DATE OF BIRTH = <!--Birth date has been contested. Do not add without providing a reliably published source with a reputation for editorial oversight--> | PLACE OF BIRTH = India | DATE OF DEATH = | PLACE OF DEATH = }} {{DEFAULTSORT:Khan, Shavez}} [[Category:Living people]] [[Category:Indian male television actors]] [[Category:Actors in Hindi television]] [[Category:Indian television personalities]]\"\n",
    "\n",
    "parse_links(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3ad09f-c84c-495c-8b4b-ef9a25d00675",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Now we use the above function to create a first forward DF and change the text column with only the links\n",
    "TempForwardDF=PartialWikipediaDF.select(\"title\",\"id\",parse_links_udf(\"text\").alias(\"links\"))\n",
    "display(TempForwardDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55cfa5d-fd38-43ef-8c51-82bcf420e267",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.2 Mapping titles to IDs\n",
    "\n",
    "The last step is to be able to map the document titles to their ids to deal with numbers instead of dealing with strings1. In order to do so, we will create a pandas DF containing 2 values; document title and document ID. With this, we can create a new function that given the document title, retrieves its ID. Putting it all together, we will be able to create a new *Forward table* containing document title, document id, and **links**, a list of IDs of linked documents.\n",
    "\n",
    "As title_id DF will be used in many distributed machines, having it as a read-only variable in all of these will help speed up the processing. For this reason, we will broadcast this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172bf1cc-a730-4794-b9ea-cffc8ef92205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the title-id dataframe\n",
    "title_idDF=PartialWikipediaDF.select(lower(\"title\").alias(\"title\"),\"id\")\n",
    "title_idPDF=title_idDF.toPandas()\n",
    "\n",
    "print(len(title_idPDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1191f9a-f8d8-4f8f-9178-b59e251186ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "broadcast_title_idPDF = sc.broadcast(title_idPDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13cbee1a-79d7-4ef1-8ea8-3f5253f7d2fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the mapping\n",
    "def titles2id(links,titleidPDF):\n",
    "    data_titles=broadcast_title_idPDF.value\n",
    "    if (len(links)>0):\n",
    "        ids=data_titles[data_titles.title.isin(links)].id.to_list()\n",
    "    else:\n",
    "        ids=[]\n",
    "    return list(set(ids))\n",
    "\n",
    "titles2id_UDF=udf(lambda x: titles2id(x,title_idPDF),ArrayType(LongType(),False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac30aae2-ed2a-4ce8-9eb0-2e69e6c4d71f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Modify the forwatd DF to include linked IDs rather than titles\n",
    "ForwardDF=TempForwardDF.select(\"id\",titles2id_UDF(\"links\").alias(\"links\")).cache()\n",
    "display(ForwardDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfba89c1-38dd-4358-80a3-a2a84ca921d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that as we are using a portion of the Wikipedia dataset some of the links entries are an empty list. This happens because in the portion of the dataset we are using those outgoing links that do not have a matching with an id. This problem will be solved latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd4ebfc-b3d5-417a-83d9-5dddbf52ee72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Creating Other Tables\n",
    "\n",
    "In order to be able to execute the page rank algorithm, we need 2 other tables. Namely, an *outgoing links table*, in which for each document ID we count how many external references we have, and a *reverse table*, in which for each document ID we take all the document ids that are referencing back to it.\n",
    "\n",
    "### 3.1 Outgoing Links\n",
    "\n",
    "We simply use the *forward table*, and compute the size of the variable links which is a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed0083e-5e0e-4601-a5a5-0d928a7a7c0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OutgoingsLinksCountersDF=ForwardDF.select(\"id\",size(\"links\").alias(\"counter\"))\n",
    "OutgoingsLinksCountersPDF=OutgoingsLinksCountersDF.toPandas()\n",
    "\n",
    "OutgoingsLinksCountersPDF.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf81a107-ee83-4e29-99d2-c7e1b470b881",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.2 Reverse Links\n",
    "\n",
    "We make use of the Forward table. We explode the links column to obtain a single relation between each pair of documents (link, t_link). We then group by **t_link**, creating a list with all instances associated to it. We rename the first as **id**, and the latter as **links**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc22c8b3-df0d-4268-b46e-e4833b72ecab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TemporalReverseLinks=ForwardDF.select(\"id\",explode(\"links\").alias(\"t_link\"))\n",
    "ReverseDF=TemporalReverseLinks.groupBy(\"t_link\").agg(collect_list (\"id\").alias(\"counters\")).cache()\n",
    "ReverseDF = ReverseDF.withColumnRenamed(\"t_link\",\"id\").withColumnRenamed(\"counters\",\"links\")\n",
    "\n",
    "ReverseDF.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f6220f5-eac5-4034-a8a0-b332ca5dc62e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lastly, we want to unify these tables for convenience. That is, we would like to add a new column to *ReverseDF*, in which we had a list with the number of **outgoing** links in each of the documents referred to in the column **links**. Those with no outgoing links will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ba0312-0730-4176-8935-5b8ca19847e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def find_outgoing(links):\n",
    "    if (len(links)>0):\n",
    "        outgoing=OutgoingsLinksCountersPDF[OutgoingsLinksCountersPDF.id.isin(links)].counter.to_list()\n",
    "    else:\n",
    "        outgoing = []      \n",
    "    \n",
    "    return list(set(outgoing)) \n",
    "\n",
    "find_outgoing_UDF=udf(lambda x: find_outgoing(x),ArrayType(LongType(),False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78506c65-4da0-4bc1-a4e7-902072daeaba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ReverseDF=ReverseDF.select(\"id\",\"links\", find_outgoing_UDF(\"links\").alias(\"outgoing\")).cache()\n",
    "ReverseDF.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c9d53d-c447-4be2-87f8-e5276861ac62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We have to remove the outgoings which contains a value of zero to avoid obtaining infinites in the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a53418d-2601-4df4-a393-de8b80e31525",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_zeros(arr):\n",
    "    return [x for x in arr if x != 0]\n",
    "\n",
    "remove_zeros_udf = udf(remove_zeros, ArrayType(LongType(),False))\n",
    "\n",
    "ReverseDF = ReverseDF.withColumn(\"outgoing\", remove_zeros_udf(\"outgoing\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc087af2-8c6b-4360-8d73-58c1047029ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We have noticed that some of the values in the **ForwardDF** do not have any documents referencing them in our data scope, and therefore they do not appear in the **ReverseDF** table. This can be a possible source of problems moving on, so we decide to fix this by making sure all ids in the forward table are included in the reverse one. For the values of links and outgoing, we will use '[]' (empty list). In this way we are able to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d40da63-86f5-4c60-bfa9-dc9522969a7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Lastly, we create a pandas DF for both forward and reverse tables. Add needed entries to reverse table too\n",
    "ReversePDF = ReverseDF.toPandas()\n",
    "ForwardPDF = ForwardDF.toPandas()\n",
    "\n",
    "for i in ForwardPDF['id']:\n",
    "    if i not in list(ReversePDF['id']):\n",
    "        ReversePDF.loc[len(ReversePDF)]= [i, [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f6ca6e7-e946-493c-8ac2-d5132712e26b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ReversePDF.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc8f479-bfce-4db6-b1b1-a9c36b9f41e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. PageRank \n",
    "\n",
    "The first step is to create the structure for our dataframe. For this, we assign to each link in **ReverseDF** a weight of 0.85/N, where N is now the number of documents considered (length of reverse table). We use 0.85 and not 1 in order to consider the damping factor, which it represents the probability that a user will continue clicking on links rather than jumping to a new page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30068252-8cdb-44a6-bf97-a305418c5f96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pageRankPDF=ReversePDF.copy()\n",
    "\n",
    "N  =  len(ReversePDF)\n",
    "pageRankPDF[\"pagerank\"]= 0.85/N "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbacd64-1a30-48cd-b3c7-adf4eae49e83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we have the basic structure for our results, we have to create an algorithm that iteratively updates values until convergence or max iterations are reached. In order to make the process straightfoward, we create 2 auxiliary functions:\n",
    "\n",
    "* The first searches for all nodes with no incoming links (dangling nodes). In these nodes, we distribute their PR to all other documents in the network. The function sums all the values to distribute (total re-distribution added to each new PR)\n",
    "\n",
    "* The second computes the new PR of all nodes in each iteration. It takes into account the function above, and applies the basic formula to all documents that do have incoming links. Also, it applies a damping factor of 0.85.\n",
    "\n",
    "Once we have this, the loop is straightforward. For max_iterations, it:\n",
    "* Creates a Spark DF with the updated information (same structure as ReverseDF)\n",
    "* Calculates the total weight to re-distribute\n",
    "* Uses the above to calculate new PR\n",
    "* Computes difference in PR for all nodes. Takes the maximum. If above threshold, we continue. Else, we break the loop.\n",
    "* We update the initial Spark DF (**resultDF**) with the new PRs and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b327721-e74c-4c73-81e1-225e776ee177",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def aux_find_dangling(links, pr, N):\n",
    "    if len(links) == 0:\n",
    "        return (pr/N)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "udf_aux_find_dangling = udf(lambda l,  pr: aux_find_dangling(l,pr,N), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841b740b-52b6-4061-8382-9a8b6259396a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def aux_PR(links, outgoing, pageRankPDF, N, redistribute):\n",
    "    #We start of each new PR with the value we need to re-distribute because of the solitary nodes in the network.\n",
    "    new_pr = redistribute\n",
    "    \n",
    "    if len(links) != 0:\n",
    "        for l in range(len(outgoing)):\n",
    "            l_pr = pageRankPDF.loc[pageRankPDF[\"id\"] == links[l], 'pagerank'].values[0]\n",
    "            l_outgoing = outgoing[l]\n",
    "            new_pr += l_pr/l_outgoing       \n",
    "        \n",
    "    new_pr = (0.85/N) + (0.15*new_pr)\n",
    "    return float(new_pr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e6dbb0-5b21-4ed4-bdeb-b93d974fe127",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def new_pagerank(pageRankPDF, tsh, max_iterations = 20):\n",
    "        \n",
    "    stop = False\n",
    "    iter_ = 0\n",
    "\n",
    "    while stop == False:\n",
    "        resultDF = sqlContext.createDataFrame(pageRankPDF)\n",
    "        \n",
    "        # Calculate weight to redistribute across network\n",
    "        r = resultDF.select(udf_aux_find_dangling(\"links\", \"pagerank\").alias('dangling')).toPandas()['dangling'].sum()\n",
    "        udf_aux_PR = udf(lambda l, c: aux_PR(l,c, pageRankPDF, N, r), FloatType())\n",
    "        \n",
    "        #Computing new rank\n",
    "        NewPageRankDF = resultDF.withColumn(\"NewPR\", udf_aux_PR(\"links\", \"outgoing\")) \n",
    "\n",
    "        #New column with difference between PR and NewPR\n",
    "        NewPageRankDF = NewPageRankDF.withColumn('Tsh', abs(col(\"pagerank\") - col('NewPR')))\n",
    "\n",
    "        max_diff = NewPageRankDF.agg(max(\"Tsh\")).collect()[0][0]        \n",
    "        iter_ +=1\n",
    "        \n",
    "        if iter_ == max_iterations or max_diff < tsh:\n",
    "            stop = True\n",
    "        \n",
    "        pageRankPDF = NewPageRankDF.select(\"id\", \"links\", \"outgoing\", \"NewPR\").withColumnRenamed(\"NewPR\",\"pagerank\").toPandas()\n",
    "\n",
    "    return (pageRankPDF, iter_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47c78ef-df37-4a22-ac3f-a509f2f59373",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "res, iters = new_pagerank(pageRankPDF, 0.0000000001) # The second argument is the tsh we are going to use\n",
    "display(res)\n",
    "print('We have converged in ', iters, 'iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf5a7fe-1648-4e0b-b454-081043b762ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Final Results\n",
    "\n",
    "The last step is to return the results in the desired format. That is, a pandas Df containing document title, ID, and its pagerank. We will use some of the previously defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d8ad38-3b12-4173-aa89-de7740e48a5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the mapping\n",
    "def id2titles(id):\n",
    "    data_titles=broadcast_title_idPDF.value\n",
    "    \n",
    "    title = data_titles[data_titles['id'] == id].title.values[0]\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23e356b-a3cf-4b7b-aa4e-b0a548234c07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "res['title'] = res['id'].map(lambda x: id2titles(x))\n",
    "res_final = res[['title', 'id', 'pagerank']]\n",
    "res_final.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42d9a68-e013-456e-8e84-465572d94fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "suma_total = res['pagerank'].sum()\n",
    "print(suma_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d591d593-5f99-4372-9111-b1b836b1da3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that the sum of the Page rank values is almost 1, so we have checked that the algorithm is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c6e4b4-b4fb-4dee-bdac-57286c97360e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## CONCLUSIONS - Daniel Toribio (100454242)\n",
    "\n",
    "The PageRank algorithm played a crucial role in the initial deployment of the Google Search Engine, establishing itself as the cornerstone of this practical endeavor. The fundamental concept involved assigning a significance to each webpage, where pages garnered more significance if they were frequently referenced by other noteworthy pages. Page rank gauges the likelihood of a random internet user arriving at a specific webpage. The probability of a user landing on a particular page rises with the number of pages linked to it. However, we encountered challenges that required resolution, including the presence of dangling nodes (pages lacking links) and the necessity to depict the likelihood of a web user's search (the damping factor indicating the probability of the user continuing to browse). To address the issue of dangling nodes and acknowledge that a user can persist in searching even when encountering a dead end, the page rank of these nodes is distributed among the remaining documents. This is achieved, for instance, through initiating a new search rather than clicking on a link.\n",
    "\n",
    "Concerning the implementation process, we initiated by employing regular expressions to extract the titles of the linked documents. Subsequently, we transformed these titles into IDs through a supplementary function, optimizing the process with the utilization of sc.broadcast for enhanced speed. This led to the creation of ForwardDF. The next step involved determining the count of outgoing links for each document, consolidating all the data into a unified dataframe named ReverseDF.\n",
    "\n",
    "Following this consolidation, we sought all the outgoing links from each ID, excluding the documents pointing to them. The page rank was then initialized and iterated until convergence. To update the pagerank, we devised two auxiliary functions. Initially, we computed the total weight that necessitated redistribution due to dangling nodes. Subsequently, the second function utilized the fundamental formula outlined in the original paper, commencing with the initialization of the new page rank to the previously calculated value. The damping factor was set at 0.85.\n",
    "\n",
    "Once the largest page rank change across all nodes was computed using the new page rank, we assessed whether it fell below our predetermined threshold. In the event that this criterion was met or the maximum number of iterations had been reached, the loop concluded. This iterative process allowed for the efficient computation of the page rank until the desired convergence was achieved.\n",
    "\n",
    "In the context of Spark's capabilities, two notable examples include the effective use of user-defined functions, particularly in the generation of **ReverseDF**, where they significantly contributed to expediting the process. Despite these advantages, there were instances where the computational time was extensive, necessitating the use of pandas' dataframes.\n",
    "\n",
    "In summary, this practical work has served as a valuable opportunity to enhance our understanding of Spark's capabilities, especially when dealing with substantial volumes of data. The utilization of user-defined functions within Spark has proven instrumental in optimizing performance. While Spark excelled in many aspects, the integration of pandas' dataframes in specific scenarios underscored the need for flexibility in handling diverse computational demands.\n",
    "\n",
    "This experience has reinforced our confidence in Spark as a potent tool for managing extensive datasets, and we anticipate leveraging its capabilities in future projects with similar requirements. Furthermore, delving deeper into the PageRank algorithm has been an enriching experience, providing insights into its significance and practical applications. The knowledge gained from this endeavor positions us well for addressing similar challenges in the future."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PageRank_FinalWork",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
